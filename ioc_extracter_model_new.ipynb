{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModel, AutoConfig,\n",
        "    TrainingArguments, Trainer,\n",
        "    DataCollatorForTokenClassification,\n",
        "    get_cosine_schedule_with_warmup\n",
        ")\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "import pickle\n",
        "from dataclasses import dataclass\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import math\n",
        "\n",
        "# Install pytorch-crf if needed\n",
        "# pip install pytorch-crf\n",
        "\n",
        "try:\n",
        "    from torchcrf import CRF\n",
        "    CRF_AVAILABLE = True\n",
        "except ImportError:\n",
        "    CRF_AVAILABLE = False\n",
        "    print(\"Warning: pytorch-crf not installed. CRF layer will be disabled.\")\n",
        "    print(\"Install with: pip install pytorch-crf\")\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2peC3OgCmLJ",
        "outputId": "db0980b1-c4fc-4995-dcc7-67449f193f43"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: pytorch-crf not installed. CRF layer will be disabled.\n",
            "Install with: pip install pytorch-crf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['WANDB_DISABLED'] = 'true'\n",
        "os.environ['WANDB_MODE'] = 'disabled'"
      ],
      "metadata": {
        "id": "HkXZmXdPEIFy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cZ9mRI8tCa-M"
      },
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"Focal Loss implementation to handle class imbalance\"\"\"\n",
        "    def __init__(self, alpha=1.0, gamma=2.0, ignore_index=-100, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.ignore_index = ignore_index\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, ignore_index=self.ignore_index, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss\n",
        "\n",
        "class LabelSmoothingLoss(nn.Module):\n",
        "    \"\"\"Label Smoothing Loss for regularization\"\"\"\n",
        "    def __init__(self, num_classes, smoothing=0.1, ignore_index=-100):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.smoothing = smoothing\n",
        "        self.ignore_index = ignore_index\n",
        "        self.confidence = 1.0 - smoothing\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        pred = pred.log_softmax(dim=-1)\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.zeros_like(pred)\n",
        "            true_dist.fill_(self.smoothing / (self.num_classes - 1))\n",
        "            mask = target != self.ignore_index\n",
        "            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n",
        "            true_dist = true_dist * mask.unsqueeze(1).float()\n",
        "\n",
        "        return torch.mean(torch.sum(-true_dist * pred, dim=-1))\n",
        "\n",
        "class IOCDataProcessor:\n",
        "    \"\"\"Enhanced IOC data processor for new dataset format\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Updated entity types to match new dataset\n",
        "        self.entity_types = [\n",
        "            # Technical IOCs\n",
        "            'IP', 'Domain', 'URL', 'File', 'Email', 'Vulnerability',\n",
        "            # Semantic Entities\n",
        "            'Type', 'Device', 'Vendor', 'Version', 'Software',\n",
        "            'Function', 'Platform', 'Malware', 'ThreatActor', 'Other'\n",
        "        ]\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.label_to_id = {}\n",
        "        self.id_to_label = {}\n",
        "        self.label_distribution = Counter()\n",
        "\n",
        "        # Updated entity mapping for new dataset format\n",
        "        self.entity_mapping = {\n",
        "            'IP': 'IP',\n",
        "            'Domain': 'DOMAIN',\n",
        "            'URL': 'URL',\n",
        "            'File': 'FILE',\n",
        "            'Email': 'EMAIL',\n",
        "            'Vulnerability': 'VULNERABILITY',\n",
        "            'Type': 'TYPE',\n",
        "            'Device': 'DEVICE',\n",
        "            'Vendor': 'VENDOR',\n",
        "            'Version': 'VERSION',\n",
        "            'Software': 'SOFTWARE',\n",
        "            'Function': 'FUNCTION',\n",
        "            'Platform': 'PLATFORM',\n",
        "            'Malware': 'MALWARE',\n",
        "            'ThreatActor': 'THREATACTOR',\n",
        "            'Other': 'OTHER'\n",
        "        }\n",
        "\n",
        "    def load_dataset(self, json_path: str) -> List[Dict]:\n",
        "        \"\"\"Load the enhanced IOC dataset\"\"\"\n",
        "        logger.info(f\"Loading dataset from {json_path}\")\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        logger.info(f\"Loaded {len(data)} samples\")\n",
        "        return data\n",
        "\n",
        "    def find_entity_positions(self, text: str, entity_value: str) -> List[Tuple[int, int]]:\n",
        "        \"\"\"Find all positions of an entity value in text with improved matching\"\"\"\n",
        "        positions = []\n",
        "        text_lower = text.lower()\n",
        "        entity_lower = entity_value.lower().strip()\n",
        "\n",
        "        # Handle multi-word entities\n",
        "        entity_clean = ' '.join(entity_lower.split())\n",
        "\n",
        "        start = 0\n",
        "        while True:\n",
        "            pos = text_lower.find(entity_clean, start)\n",
        "            if pos == -1:\n",
        "                break\n",
        "\n",
        "            # Verify word boundaries for better matching\n",
        "            is_start_valid = pos == 0 or text[pos-1].isspace() or text[pos-1] in '.,;:()[]{}\"\\''\n",
        "            end_pos = pos + len(entity_value)\n",
        "            is_end_valid = end_pos >= len(text) or text[end_pos].isspace() or text[end_pos] in '.,;:()[]{}\"\\''\n",
        "\n",
        "            if is_start_valid and is_end_valid:\n",
        "                positions.append((pos, pos + len(entity_value)))\n",
        "\n",
        "            start = pos + 1\n",
        "\n",
        "        return positions\n",
        "\n",
        "    def clean_entity_value(self, entity_value: str, entity_type: str) -> str:\n",
        "        \"\"\"Clean and normalize entity values\"\"\"\n",
        "        if not entity_value or entity_value == \"NULL\":\n",
        "            return None\n",
        "\n",
        "        cleaned = entity_value.strip()\n",
        "\n",
        "        # Remove common prefixes\n",
        "        if \":\" in cleaned:\n",
        "            parts = cleaned.split(\":\", 1)\n",
        "            if parts[0] in self.entity_types:\n",
        "                cleaned = parts[1].strip()\n",
        "\n",
        "        # Handle specific cleaning based on entity type\n",
        "        if entity_type == \"File\":\n",
        "            if cleaned.startswith(\"FileHash-\"):\n",
        "                if \":\" in cleaned:\n",
        "                    cleaned = cleaned.split(\":\", 1)[1].strip()\n",
        "\n",
        "        # NEW: Remove trailing punctuation that shouldn't be part of entities\n",
        "        if entity_type in [\"Domain\", \"URL\", \"Email\"]:\n",
        "            cleaned = cleaned.rstrip('.,;:')\n",
        "\n",
        "        # NEW: Handle multi-word entities better (especially for ThreatActor)\n",
        "        if entity_type == \"ThreatActor\":\n",
        "            # Keep full multi-word names\n",
        "            cleaned = ' '.join(cleaned.split())\n",
        "\n",
        "        return cleaned if cleaned else None\n",
        "\n",
        "    def convert_new_format_to_bio(self, text: str, entities_dict: Dict) -> List[str]:\n",
        "        \"\"\"Convert new dataset format to BIO tagging format\"\"\"\n",
        "        tokens = text.split()\n",
        "        bio_tags = ['O'] * len(tokens)\n",
        "\n",
        "        # Create character to token mapping\n",
        "        char_to_token = {}\n",
        "        char_pos = 0\n",
        "        for token_idx, token in enumerate(tokens):\n",
        "            while char_pos < len(text) and text[char_pos].isspace():\n",
        "                char_pos += 1\n",
        "            for i in range(len(token)):\n",
        "                if char_pos + i < len(text):\n",
        "                    char_to_token[char_pos + i] = token_idx\n",
        "            char_pos += len(token)\n",
        "\n",
        "        # Process each entity type\n",
        "        all_entities = []\n",
        "        for entity_type, entity_values in entities_dict.items():\n",
        "            if entity_type not in self.entity_mapping:\n",
        "                continue\n",
        "\n",
        "            mapped_type = self.entity_mapping[entity_type]\n",
        "\n",
        "            # Handle both list and single value formats\n",
        "            if not entity_values or entity_values == \"NULL\" or entity_values == [\"NULL\"]:\n",
        "                continue\n",
        "\n",
        "            # Ensure entity_values is a list\n",
        "            if not isinstance(entity_values, list):\n",
        "                entity_values = [entity_values]\n",
        "\n",
        "            for entity_value in entity_values:\n",
        "                cleaned_value = self.clean_entity_value(entity_value, entity_type)\n",
        "\n",
        "                if not cleaned_value:\n",
        "                    continue\n",
        "\n",
        "                # Find positions in text\n",
        "                positions = self.find_entity_positions(text, cleaned_value)\n",
        "                for start_char, end_char in positions:\n",
        "                    all_entities.append({\n",
        "                        'start_pos': start_char,\n",
        "                        'end_pos': end_char,\n",
        "                        'entity_type': mapped_type,\n",
        "                        'entity_value': cleaned_value\n",
        "                    })\n",
        "\n",
        "        # Sort entities by start position to handle overlaps\n",
        "        sorted_entities = sorted(all_entities, key=lambda x: x['start_pos'])\n",
        "\n",
        "        # Convert to BIO tags with overlap handling\n",
        "        used_tokens = set()\n",
        "        for entity in sorted_entities:\n",
        "            start_char = entity['start_pos']\n",
        "            end_char = entity['end_pos']\n",
        "            entity_type = entity['entity_type']\n",
        "\n",
        "            # Find corresponding tokens\n",
        "            start_token = None\n",
        "            end_token = None\n",
        "\n",
        "            # Find start token\n",
        "            for offset in range(5):  # Look ahead up to 5 characters\n",
        "                if start_char + offset in char_to_token:\n",
        "                    start_token = char_to_token[start_char + offset]\n",
        "                    break\n",
        "\n",
        "            # Find end token\n",
        "            for offset in range(5):  # Look back up to 5 characters\n",
        "                if end_char - 1 - offset in char_to_token:\n",
        "                    end_token = char_to_token[end_char - 1 - offset]\n",
        "                    break\n",
        "\n",
        "            if start_token is not None and end_token is not None:\n",
        "                # Skip if tokens are already used (handle overlaps)\n",
        "                if start_token in used_tokens:\n",
        "                    continue\n",
        "\n",
        "                if start_token == end_token:\n",
        "                    if bio_tags[start_token] == 'O':\n",
        "                        bio_tags[start_token] = f'B-{entity_type}'\n",
        "                        used_tokens.add(start_token)\n",
        "                else:\n",
        "                    if bio_tags[start_token] == 'O':\n",
        "                        bio_tags[start_token] = f'B-{entity_type}'\n",
        "                        used_tokens.add(start_token)\n",
        "                    for token_idx in range(start_token + 1, min(end_token + 1, len(bio_tags))):\n",
        "                        if bio_tags[token_idx] == 'O' and token_idx not in used_tokens:\n",
        "                            bio_tags[token_idx] = f'I-{entity_type}'\n",
        "                            used_tokens.add(token_idx)\n",
        "\n",
        "        return bio_tags\n",
        "\n",
        "    def prepare_training_data(self, dataset: List[Dict]) -> Tuple[List[str], List[List[str]]]:\n",
        "        \"\"\"Prepare texts and BIO tags for training with label distribution analysis\"\"\"\n",
        "        texts = []\n",
        "        all_tags = []\n",
        "        skipped = 0\n",
        "\n",
        "        for sample in dataset:\n",
        "            text = sample['text']\n",
        "            entities_dict = sample['entities']\n",
        "\n",
        "            # Skip very long texts\n",
        "            if len(text.split()) > 450:\n",
        "                skipped += 1\n",
        "                continue\n",
        "\n",
        "            bio_tags = self.convert_new_format_to_bio(text, entities_dict)\n",
        "\n",
        "            # Update label distribution\n",
        "            self.label_distribution.update(bio_tags)\n",
        "\n",
        "            # Include samples with at least one entity OR with meaningful content\n",
        "            if any(tag != 'O' for tag in bio_tags):\n",
        "                texts.append(text)\n",
        "                all_tags.append(bio_tags)\n",
        "\n",
        "        # Log statistics\n",
        "        logger.info(f\"Skipped {skipped} samples (too long)\")\n",
        "        logger.info(f\"Prepared {len(texts)} training samples\")\n",
        "\n",
        "        # Log label distribution\n",
        "        logger.info(\"\\nLabel Distribution Analysis:\")\n",
        "        total_labels = sum(self.label_distribution.values())\n",
        "        for label, count in self.label_distribution.most_common():\n",
        "            percentage = (count / total_labels) * 100\n",
        "            logger.info(f\"  {label}: {count} ({percentage:.2f}%)\")\n",
        "\n",
        "        return texts, all_tags\n",
        "\n",
        "    def create_label_mappings(self, all_tags: List[List[str]]):\n",
        "        \"\"\"Create label to ID mappings\"\"\"\n",
        "        unique_labels = set()\n",
        "        for tags in all_tags:\n",
        "            unique_labels.update(tags)\n",
        "\n",
        "        unique_labels = sorted(list(unique_labels))\n",
        "        self.label_to_id = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "        self.id_to_label = {idx: label for label, idx in self.label_to_id.items()}\n",
        "\n",
        "        logger.info(f\"\\nCreated mappings for {len(unique_labels)} labels:\")\n",
        "        for label in unique_labels:\n",
        "            logger.info(f\"  {label}: {self.label_to_id[label]}\")\n",
        "\n",
        "        return unique_labels\n",
        "\n",
        "    def get_class_weights(self, all_tags: List[List[str]]) -> torch.Tensor:\n",
        "        \"\"\"Calculate class weights for handling imbalanced data\"\"\"\n",
        "        flat_tags = [tag for tags in all_tags for tag in tags]\n",
        "        unique_labels = list(self.label_to_id.keys())\n",
        "        label_counts = [flat_tags.count(label) for label in unique_labels]\n",
        "\n",
        "        total_samples = len(flat_tags)\n",
        "        weights = []\n",
        "\n",
        "        for i, count in enumerate(label_counts):\n",
        "            if count > 0:\n",
        "                # NEW: More aggressive weighting for rare classes\n",
        "                if unique_labels[i] == 'O':\n",
        "                    weight = total_samples / (len(unique_labels) * count * 5)  # Changed from 3 to 5\n",
        "                elif count < 100:  # NEW: Boost very rare classes\n",
        "                    weight = total_samples / (len(unique_labels) * count * 0.2)  # Stronger boost\n",
        "                else:\n",
        "                    weight = total_samples / (len(unique_labels) * count * 0.5)\n",
        "            else:\n",
        "                weight = 1.0\n",
        "            weights.append(weight)\n",
        "\n",
        "        logger.info(\"\\nClass weights calculated:\")\n",
        "        for label, weight in zip(unique_labels, weights):\n",
        "            logger.info(f\"  {label}: {weight:.4f}\")\n",
        "\n",
        "        return torch.FloatTensor(weights)\n",
        "\n",
        "class IOCDataset(Dataset):\n",
        "    \"\"\"Enhanced PyTorch Dataset for IOC extraction\"\"\"\n",
        "\n",
        "    def __init__(self, texts: List[str], tags: List[List[str]],\n",
        "                 tokenizer, label_to_id: Dict, max_length: int = 512):\n",
        "        self.texts = texts\n",
        "        self.tags = tags\n",
        "        self.tokenizer = tokenizer\n",
        "        self.label_to_id = label_to_id\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        tags = self.tags[idx]\n",
        "\n",
        "        # Tokenize\n",
        "        encoding = self.tokenizer(\n",
        "            text.split(),\n",
        "            is_split_into_words=True,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Align labels with tokens\n",
        "        word_ids = encoding.word_ids()\n",
        "        aligned_labels = []\n",
        "\n",
        "        for word_id in word_ids:\n",
        "            if word_id is None:\n",
        "                aligned_labels.append(-100)\n",
        "            elif word_id >= len(tags):\n",
        "                aligned_labels.append(self.label_to_id['O'])\n",
        "            else:\n",
        "                aligned_labels.append(self.label_to_id[tags[word_id]])\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(aligned_labels, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "class EnhancedIOCExtractionModel(nn.Module):\n",
        "    \"\"\"Enhanced DeBERTa-v3-based IOC extraction model\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str, num_labels: int, dropout_rate: float = 0.1,\n",
        "                 use_crf: bool = False, class_weights: Optional[torch.Tensor] = None,\n",
        "                 loss_type: str = 'focal'):\n",
        "        super().__init__()\n",
        "        self.deberta = AutoModel.from_pretrained(model_name)\n",
        "        self.num_labels = num_labels\n",
        "        self.use_crf = use_crf and CRF_AVAILABLE\n",
        "        self.loss_type = loss_type\n",
        "\n",
        "        hidden_size = self.deberta.config.hidden_size\n",
        "        # Three-layer classifier with more capacity\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
        "\n",
        "        # NEW: Larger intermediate layer\n",
        "        self.classifier_1 = nn.Linear(hidden_size, hidden_size)  # Changed from hidden_size//2\n",
        "        self.classifier_1_norm = nn.LayerNorm(hidden_size)\n",
        "        self.classifier_1_dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # NEW: Additional layer\n",
        "        self.classifier_2 = nn.Linear(hidden_size, hidden_size // 2)\n",
        "        self.classifier_2_norm = nn.LayerNorm(hidden_size // 2)\n",
        "        self.classifier_2_dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.classifier_3 = nn.Linear(hidden_size // 2, num_labels)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "        if self.use_crf:\n",
        "            self.crf = CRF(num_labels, batch_first=True)\n",
        "            logger.info(\"CRF layer enabled\")\n",
        "\n",
        "        if class_weights is not None:\n",
        "            self.register_buffer('class_weights', class_weights)\n",
        "        else:\n",
        "            self.class_weights = None\n",
        "\n",
        "        if loss_type == 'focal':\n",
        "            self.loss_fn = FocalLoss(alpha=1.0, gamma=2.0, ignore_index=-100)\n",
        "        elif loss_type == 'label_smoothing':\n",
        "            self.loss_fn = LabelSmoothingLoss(num_labels, smoothing=0.1, ignore_index=-100)\n",
        "        else:\n",
        "            self.loss_fn = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-100)\n",
        "\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize classifier weights using Xavier uniform\"\"\"\n",
        "        nn.init.xavier_uniform_(self.classifier_1.weight)\n",
        "        nn.init.zeros_(self.classifier_1.bias)\n",
        "        nn.init.xavier_uniform_(self.classifier_2.weight)\n",
        "        nn.init.zeros_(self.classifier_2.bias)\n",
        "        nn.init.xavier_uniform_(self.classifier_3.weight)  # NEW\n",
        "        nn.init.zeros_(self.classifier_3.bias)  # NEW\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None, **kwargs):\n",
        "        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs.last_hidden_state\n",
        "\n",
        "        sequence_output = self.layer_norm(sequence_output)\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "\n",
        "        hidden = self.classifier_1(sequence_output)\n",
        "        hidden = F.gelu(hidden)\n",
        "        hidden = self.classifier_1_norm(hidden)\n",
        "        hidden = self.classifier_1_dropout(hidden)\n",
        "\n",
        "        # NEW: Second hidden layer\n",
        "        hidden = self.classifier_2(hidden)\n",
        "        hidden = F.gelu(hidden)\n",
        "        hidden = self.classifier_2_norm(hidden)\n",
        "        hidden = self.classifier_2_dropout(hidden)\n",
        "\n",
        "        logits = self.classifier_3(hidden)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.use_crf:\n",
        "                mask = attention_mask.bool()\n",
        "                crf_labels = labels.clone()\n",
        "                crf_labels[labels == -100] = 0\n",
        "                loss = -self.crf(logits, crf_labels, mask=mask, reduction='mean')\n",
        "            else:\n",
        "                loss = self.loss_fn(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        return {'loss': loss, 'logits': logits}\n",
        "\n",
        "    def decode_predictions(self, logits, attention_mask):\n",
        "        \"\"\"Decode predictions using CRF if available\"\"\"\n",
        "        if self.use_crf:\n",
        "            mask = attention_mask.bool()\n",
        "            batch_size = mask.size(0)\n",
        "            for i in range(batch_size):\n",
        "                mask[i, 0] = True\n",
        "            predictions = self.crf.decode(logits, mask=mask)\n",
        "            return predictions\n",
        "        else:\n",
        "            return torch.argmax(logits, dim=-1).tolist()\n",
        "\n",
        "# [Rest of the code remains the same: EnhancedIOCModelTrainer, EnhancedIOCModelInference, and main() function]\n",
        "# The key changes are in IOCDataProcessor class\n",
        "\n",
        "class EnhancedIOCModelTrainer:\n",
        "    \"\"\"Enhanced training class\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"microsoft/deberta-v3-base\"):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.processor = IOCDataProcessor()\n",
        "        self.model = None\n",
        "\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "    def train(self,\n",
        "              dataset_path: str,\n",
        "              output_dir: str = \"enhanced_ioc_deberta_model\",\n",
        "              test_size: float = 0.2,\n",
        "              batch_size: int = 8,\n",
        "              num_epochs: int = 6,\n",
        "              learning_rate: float = 2e-5,\n",
        "              use_crf: bool = False,\n",
        "              loss_type: str = 'focal'):\n",
        "        \"\"\"Train the enhanced IOC extraction model\"\"\"\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Load and process data\n",
        "        dataset = self.processor.load_dataset(dataset_path)\n",
        "        texts, all_tags = self.processor.prepare_training_data(dataset)\n",
        "        unique_labels = self.processor.create_label_mappings(all_tags)\n",
        "\n",
        "        # Calculate class weights\n",
        "        class_weights = self.processor.get_class_weights(all_tags)\n",
        "\n",
        "        # Split data\n",
        "        split_idx = int(len(texts) * (1 - test_size))\n",
        "        train_texts, test_texts = texts[:split_idx], texts[split_idx:]\n",
        "        train_tags, test_tags = all_tags[:split_idx], all_tags[split_idx:]\n",
        "\n",
        "        logger.info(f\"\\nTrain samples: {len(train_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n",
        "        # Create datasets\n",
        "        train_dataset = IOCDataset(\n",
        "            train_texts, train_tags, self.tokenizer, self.processor.label_to_id, max_length=512\n",
        "        )\n",
        "        test_dataset = IOCDataset(\n",
        "            test_texts, test_tags, self.tokenizer, self.processor.label_to_id, max_length=512\n",
        "        )\n",
        "\n",
        "        # Initialize model\n",
        "        self.model = EnhancedIOCExtractionModel(\n",
        "            self.model_name,\n",
        "            len(unique_labels),\n",
        "            use_crf=use_crf,\n",
        "            class_weights=class_weights,\n",
        "            loss_type=loss_type\n",
        "        )\n",
        "        self.model.to(self.model.device)\n",
        "\n",
        "        # Training arguments\n",
        "        total_steps = len(train_dataset) // batch_size * num_epochs\n",
        "        warmup_steps = int(0.1 * total_steps)\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            num_train_epochs=num_epochs,\n",
        "            per_device_train_batch_size=batch_size,\n",
        "            per_device_eval_batch_size=batch_size,\n",
        "            learning_rate=learning_rate,\n",
        "            weight_decay=0.01,\n",
        "            warmup_steps=warmup_steps,\n",
        "            logging_steps=50,\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_loss\",\n",
        "            greater_is_better=False,\n",
        "            remove_unused_columns=False,\n",
        "            dataloader_pin_memory=False,\n",
        "            report_to=None,\n",
        "            fp16=torch.cuda.is_available(),\n",
        "            gradient_accumulation_steps=2,\n",
        "            adam_epsilon=1e-8,\n",
        "            max_grad_norm=1.0,\n",
        "            lr_scheduler_type=\"cosine\",\n",
        "            save_total_limit=3,\n",
        "            dataloader_num_workers=2,\n",
        "        )\n",
        "\n",
        "        data_collator = DataCollatorForTokenClassification(\n",
        "            tokenizer=self.tokenizer,\n",
        "            pad_to_multiple_of=8,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        class EnhancedTrainer(Trainer):\n",
        "            def log(self, logs, start_time=None):\n",
        "                super().log(logs)\n",
        "                if 'train_loss' in logs:\n",
        "                    logger.info(f\"Step {self.state.global_step}: Train Loss = {logs['train_loss']:.4f}\")\n",
        "                if 'eval_loss' in logs:\n",
        "                    logger.info(f\"Step {self.state.global_step}: Eval Loss = {logs['eval_loss']:.4f}\")\n",
        "\n",
        "        trainer = EnhancedTrainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=test_dataset,\n",
        "            data_collator=data_collator,\n",
        "            processing_class=self.tokenizer\n",
        "        )\n",
        "\n",
        "        logger.info(f\"\\nTraining Configuration:\")\n",
        "        logger.info(f\"  Model: {self.model_name}\")\n",
        "        logger.info(f\"  Loss Type: {loss_type}\")\n",
        "        logger.info(f\"  Use CRF: {use_crf}\")\n",
        "        logger.info(f\"  Batch Size: {batch_size}\")\n",
        "        logger.info(f\"  Learning Rate: {learning_rate}\")\n",
        "        logger.info(f\"  Epochs: {num_epochs}\")\n",
        "\n",
        "        logger.info(\"\\nStarting training...\")\n",
        "        try:\n",
        "            trainer.train()\n",
        "            logger.info(\"Training completed successfully!\")\n",
        "\n",
        "            final_eval = trainer.evaluate()\n",
        "            logger.info(f\"Final Evaluation Loss: {final_eval['eval_loss']:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Training failed: {e}\")\n",
        "            raise\n",
        "\n",
        "        # Save model\n",
        "        trainer.save_model()\n",
        "        self.tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "        # Save mappings\n",
        "        with open(f\"{output_dir}/label_mappings.pkl\", 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'label_to_id': self.processor.label_to_id,\n",
        "                'id_to_label': self.processor.id_to_label,\n",
        "                'entity_mapping': self.processor.entity_mapping,\n",
        "                'model_name': self.model_name,\n",
        "                'use_crf': use_crf,\n",
        "                'loss_type': loss_type,\n",
        "                'class_weights': class_weights.tolist(),\n",
        "                'label_distribution': dict(self.processor.label_distribution)\n",
        "            }, f)\n",
        "\n",
        "        # Evaluate\n",
        "        self.evaluate(test_texts, test_tags)\n",
        "\n",
        "        logger.info(f\"\\nTraining complete! Model saved to {output_dir}\")\n",
        "        return trainer.state.log_history\n",
        "\n",
        "    def evaluate(self, test_texts: List[str], test_tags: List[List[str]]):\n",
        "        \"\"\"Evaluate model with detailed metrics\"\"\"\n",
        "        logger.info(\"\\nEvaluating model...\")\n",
        "\n",
        "        predictions = []\n",
        "        true_labels = []\n",
        "        confidence_scores = []\n",
        "\n",
        "        for text, tags in zip(test_texts, test_tags):\n",
        "            try:\n",
        "                pred_entities = self.predict(text)\n",
        "                pred_tags = ['O'] * len(text.split())\n",
        "\n",
        "                for entity in pred_entities:\n",
        "                    start_token = entity.get('start_token', 0)\n",
        "                    end_token = entity.get('end_token', start_token)\n",
        "                    entity_type = entity['entity_type']\n",
        "                    confidence = entity.get('confidence', 0.0)\n",
        "\n",
        "                    if start_token < len(pred_tags) and start_token >= 0:\n",
        "                        pred_tags[start_token] = f\"B-{entity_type}\"\n",
        "                        confidence_scores.append(confidence)\n",
        "                        for i in range(start_token + 1, min(end_token + 1, len(pred_tags))):\n",
        "                            if i < len(pred_tags):\n",
        "                                pred_tags[i] = f\"I-{entity_type}\"\n",
        "                                confidence_scores.append(confidence)\n",
        "\n",
        "                min_len = min(len(pred_tags), len(tags))\n",
        "                predictions.extend(pred_tags[:min_len])\n",
        "                true_labels.extend(tags[:min_len])\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error predicting: {e}\")\n",
        "                continue\n",
        "\n",
        "        if not predictions or not true_labels:\n",
        "            logger.error(\"No valid predictions generated\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            print(\"\\nClassification Report:\")\n",
        "            report = classification_report(true_labels, predictions, output_dict=True, zero_division=0)\n",
        "            print(classification_report(true_labels, predictions, zero_division=0))\n",
        "\n",
        "            logger.info(\"\\nKey Metrics:\")\n",
        "            logger.info(f\"  Overall Accuracy: {report['accuracy']:.4f}\")\n",
        "            logger.info(f\"  Macro Avg F1: {report['macro avg']['f1-score']:.4f}\")\n",
        "            logger.info(f\"  Weighted Avg F1: {report['weighted avg']['f1-score']:.4f}\")\n",
        "\n",
        "            if confidence_scores:\n",
        "                avg_confidence = np.mean(confidence_scores)\n",
        "                logger.info(f\"  Average Confidence: {avg_confidence:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating report: {e}\")\n",
        "\n",
        "    def predict(self, text: str, threshold: float = 0.8) -> List[Dict]:\n",
        "        \"\"\"Predict entities in text\"\"\"\n",
        "        if not self.model:\n",
        "            raise ValueError(\"Model not trained yet!\")\n",
        "\n",
        "        tokens = text.split()\n",
        "        if not tokens:\n",
        "            return []\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            tokens,\n",
        "            is_split_into_words=True,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=512,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        if 'token_type_ids' in encoding:\n",
        "            del encoding['token_type_ids']\n",
        "\n",
        "        word_ids = encoding.word_ids()\n",
        "        encoding = {k: v.to(self.model.device) for k, v in encoding.items()}\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**encoding)\n",
        "\n",
        "            if self.model.use_crf:\n",
        "                predictions = self.model.decode_predictions(outputs['logits'], encoding['attention_mask'])\n",
        "                probs = torch.nn.functional.softmax(outputs['logits'], dim=-1)\n",
        "            else:\n",
        "                probs = torch.nn.functional.softmax(outputs['logits'], dim=-1)\n",
        "                predictions = torch.argmax(probs, dim=-1)\n",
        "\n",
        "        entities = []\n",
        "        current_entity = None\n",
        "\n",
        "        if self.model.use_crf:\n",
        "            pred_labels = predictions[0] if isinstance(predictions[0], list) else predictions\n",
        "        else:\n",
        "            pred_labels = predictions[0]\n",
        "            probs = probs[0]\n",
        "\n",
        "        for i, word_id in enumerate(word_ids):\n",
        "            if word_id is None or word_id >= len(tokens) or i >= len(pred_labels):\n",
        "                continue\n",
        "\n",
        "            if self.model.use_crf:\n",
        "                predicted_label_id = pred_labels[i]\n",
        "                confidence = 0.9\n",
        "            else:\n",
        "                predicted_label_id = pred_labels[i].item()\n",
        "                confidence = torch.max(probs[i]).item()\n",
        "\n",
        "            predicted_label = self.processor.id_to_label[predicted_label_id]\n",
        "\n",
        "            if predicted_label != 'O' and confidence > threshold:\n",
        "                if predicted_label.startswith('B-'):\n",
        "                    if current_entity:\n",
        "                        entities.append(current_entity)\n",
        "\n",
        "                    if word_id < len(tokens):\n",
        "                        current_entity = {\n",
        "                            'start_pos': word_id,\n",
        "                            'end_pos': word_id + 1,\n",
        "                            'entity_type': predicted_label[2:],\n",
        "                            'entity_value': tokens[word_id],\n",
        "                            'confidence': confidence,\n",
        "                            'start_token': word_id,\n",
        "                            'end_token': word_id\n",
        "                        }\n",
        "                elif predicted_label.startswith('I-') and current_entity:\n",
        "                    entity_type = predicted_label[2:]\n",
        "                    if entity_type == current_entity['entity_type'] and word_id < len(tokens):\n",
        "                        current_entity['entity_value'] += ' ' + tokens[word_id]\n",
        "                        current_entity['end_token'] = word_id\n",
        "                        current_entity['confidence'] = min(current_entity['confidence'], confidence)\n",
        "\n",
        "        if current_entity:\n",
        "            entities.append(current_entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "class EnhancedIOCModelInference:\n",
        "    \"\"\"Enhanced production inference class\"\"\"\n",
        "\n",
        "    def __init__(self, model_dir: str):\n",
        "        self.model_dir = model_dir\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "\n",
        "        # Load mappings\n",
        "        with open(f\"{model_dir}/label_mappings.pkl\", 'rb') as f:\n",
        "            mappings = pickle.load(f)\n",
        "            self.label_to_id = mappings['label_to_id']\n",
        "            self.id_to_label = mappings['id_to_label']\n",
        "            self.entity_mapping = mappings.get('entity_mapping', {})\n",
        "            self.model_name = mappings.get('model_name', 'microsoft/deberta-v3-base')\n",
        "            self.use_crf = mappings.get('use_crf', False)\n",
        "            self.loss_type = mappings.get('loss_type', 'focal')\n",
        "            self.class_weights = torch.FloatTensor(mappings.get('class_weights', []))\n",
        "\n",
        "        # Load model\n",
        "        self.model = EnhancedIOCExtractionModel(\n",
        "            model_name=self.model_name,\n",
        "            num_labels=len(self.label_to_id),\n",
        "            use_crf=self.use_crf,\n",
        "            class_weights=self.class_weights,\n",
        "            loss_type=self.loss_type\n",
        "        )\n",
        "\n",
        "        # Load weights\n",
        "        model_path = os.path.join(model_dir, \"pytorch_model.bin\")\n",
        "        if os.path.exists(model_path):\n",
        "            self.model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
        "        else:\n",
        "            model_path = os.path.join(model_dir, \"model.safetensors\")\n",
        "            if os.path.exists(model_path):\n",
        "                from safetensors.torch import load_file\n",
        "                self.model.load_state_dict(load_file(model_path))\n",
        "            else:\n",
        "                raise FileNotFoundError(f\"No model weights found in {model_dir}\")\n",
        "\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "    def extract_iocs(self, text: str, confidence_threshold: float = 0.75) -> Dict:\n",
        "        \"\"\"Extract IOCs with categorization\"\"\"\n",
        "        tokens = text.split()\n",
        "        if not tokens:\n",
        "            return {'technical_iocs': [], 'semantic_entities': [], 'text': text}\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            tokens,\n",
        "            is_split_into_words=True,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=512,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        if 'token_type_ids' in encoding:\n",
        "            del encoding['token_type_ids']\n",
        "\n",
        "        word_ids = encoding.word_ids()\n",
        "        encoding = {k: v.to(self.device) for k, v in encoding.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**encoding)\n",
        "\n",
        "            if self.model.use_crf:\n",
        "                predictions = self.model.decode_predictions(outputs['logits'], encoding['attention_mask'])\n",
        "                probs = torch.nn.functional.softmax(outputs['logits'], dim=-1)\n",
        "            else:\n",
        "                probs = torch.nn.functional.softmax(outputs['logits'], dim=-1)\n",
        "                predictions = torch.argmax(probs, dim=-1)\n",
        "\n",
        "        # Extract entities\n",
        "        entities = []\n",
        "        current_entity = None\n",
        "\n",
        "        if self.model.use_crf:\n",
        "            pred_labels = predictions[0] if isinstance(predictions[0], list) else predictions\n",
        "        else:\n",
        "            pred_labels = predictions[0]\n",
        "            probs = probs[0]\n",
        "\n",
        "        # Create word_id to token position mapping\n",
        "        word_id_to_token_pos = {}\n",
        "        for i, word_id in enumerate(word_ids):\n",
        "            if word_id is not None:\n",
        "                if word_id not in word_id_to_token_pos:\n",
        "                    word_id_to_token_pos[word_id] = []\n",
        "                word_id_to_token_pos[word_id].append(i)\n",
        "\n",
        "        processed_word_ids = set()\n",
        "\n",
        "        for i, word_id in enumerate(word_ids):\n",
        "            if word_id is None or word_id >= len(tokens) or word_id in processed_word_ids:\n",
        "                continue\n",
        "\n",
        "            processed_word_ids.add(word_id)\n",
        "            token_positions = word_id_to_token_pos[word_id]\n",
        "            first_pos = token_positions[0]\n",
        "\n",
        "            if first_pos >= len(pred_labels):\n",
        "                continue\n",
        "\n",
        "            if self.model.use_crf:\n",
        "                predicted_label_id = pred_labels[first_pos]\n",
        "                confidence = 0.9\n",
        "            else:\n",
        "                predicted_label_id = pred_labels[first_pos].item()\n",
        "                confidence = torch.max(probs[first_pos]).item()\n",
        "\n",
        "            predicted_label = self.id_to_label[predicted_label_id]\n",
        "\n",
        "            if predicted_label != 'O' and confidence > confidence_threshold:\n",
        "                if predicted_label.startswith('B-'):\n",
        "                    if current_entity:\n",
        "                        entities.append(current_entity)\n",
        "\n",
        "                    entity_type = predicted_label[2:]\n",
        "                    current_entity = {\n",
        "                        'entity_type': entity_type,\n",
        "                        'entity_value': tokens[word_id],\n",
        "                        'start_pos': len(' '.join(tokens[:word_id])) + (1 if word_id > 0 else 0),\n",
        "                        'end_pos': len(' '.join(tokens[:word_id + 1])) + (1 if word_id > 0 else 0),\n",
        "                        'confidence': confidence,\n",
        "                        'category': self._get_entity_category(entity_type)\n",
        "                    }\n",
        "                elif predicted_label.startswith('I-') and current_entity:\n",
        "                    entity_type = predicted_label[2:]\n",
        "                    if entity_type == current_entity['entity_type']:\n",
        "                        current_entity['entity_value'] += ' ' + tokens[word_id]\n",
        "                        current_entity['end_pos'] = len(' '.join(tokens[:word_id + 1])) + (1 if word_id > 0 else 0)\n",
        "                        current_entity['confidence'] = min(current_entity['confidence'], confidence)\n",
        "\n",
        "        if current_entity:\n",
        "            entities.append(current_entity)\n",
        "\n",
        "        # Separate by category\n",
        "        technical_iocs = [e for e in entities if e['category'] == 'technical_ioc']\n",
        "        semantic_entities = [e for e in entities if e['category'] == 'semantic_entity']\n",
        "\n",
        "        return {\n",
        "            'technical_iocs': technical_iocs,\n",
        "            'semantic_entities': semantic_entities,\n",
        "            'text': text,\n",
        "            'total_entities': len(entities)\n",
        "        }\n",
        "\n",
        "    def _get_entity_category(self, entity_type: str) -> str:\n",
        "        \"\"\"Categorize entity types\"\"\"\n",
        "        technical_types = {'IP', 'DOMAIN', 'URL', 'FILE', 'EMAIL', 'VULNERABILITY'}\n",
        "        if entity_type in technical_types:\n",
        "            return 'technical_ioc'\n",
        "        else:\n",
        "            return 'semantic_entity'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main training function\"\"\"\n",
        "\n",
        "    dataset_path = \"balanced_ioc_dataset.json\"\n",
        "\n",
        "    if not os.path.exists(dataset_path):\n",
        "        logger.error(f\"Dataset file {dataset_path} not found!\")\n",
        "        return\n",
        "\n",
        "    logger.info(\"=\" * 80)\n",
        "    logger.info(\"Training Enhanced IOC Extraction Model with New Dataset Format\")\n",
        "    logger.info(\"=\" * 80)\n",
        "\n",
        "    try:\n",
        "        trainer = EnhancedIOCModelTrainer(model_name=\"microsoft/deberta-v3-base\")\n",
        "\n",
        "        # Train with CRF and Focal Loss (best for imbalanced data)\n",
        "        trainer.train(\n",
        "            dataset_path=dataset_path,\n",
        "            output_dir=\"enhanced_ioc_model_v2\",\n",
        "            test_size=0.2,\n",
        "            batch_size=4,\n",
        "            num_epochs=10,  # Changed from 6 to 10\n",
        "            learning_rate=3e-5,  # Changed from 2e-5 to 3e-5\n",
        "            use_crf=True,\n",
        "            loss_type='focal'\n",
        "        )\n",
        "\n",
        "        # Test the trained model\n",
        "        logger.info(\"\\n\" + \"=\" * 80)\n",
        "        logger.info(\"Testing Model Inference\")\n",
        "        logger.info(\"=\" * 80)\n",
        "\n",
        "        inferencer = EnhancedIOCModelInference(\"enhanced_ioc_model_v2\")\n",
        "\n",
        "        # Test with sample text from new dataset format\n",
        "        test_text = \"\"\"\n",
        "        This analysis explores the infrastructure of Laundry Bear, a Russian state-sponsored APT group active since April 2024,\n",
        "        targeting NATO countries and Ukraine. The investigation reveals connections to IP addresses 154.216.18.83 and 104.36.83.170.\n",
        "        Key domains include aficors.com, aoc-gov.us, and app-v4-mybos.com. The malware uses CVE-2024-38196 for privilege escalation.\n",
        "        The attack leverages Windows components and targets Microsoft systems with malicious executables.\n",
        "        \"\"\"\n",
        "\n",
        "        results = inferencer.extract_iocs(test_text, confidence_threshold=0.6)\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"EXTRACTION RESULTS\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"Total entities extracted: {results['total_entities']}\")\n",
        "        print(f\"Technical IOCs: {len(results['technical_iocs'])}\")\n",
        "        print(f\"Semantic Entities: {len(results['semantic_entities'])}\")\n",
        "\n",
        "        if results['technical_iocs']:\n",
        "            print(f\"\\n{'Technical IOCs':^80}\")\n",
        "            print(\"-\" * 80)\n",
        "            for ioc in results['technical_iocs']:\n",
        "                print(f\"  Type: {ioc['entity_type']:<15} | Value: {ioc['entity_value']:<40} | Conf: {ioc['confidence']:.3f}\")\n",
        "\n",
        "        if results['semantic_entities']:\n",
        "            print(f\"\\n{'Semantic Entities':^80}\")\n",
        "            print(\"-\" * 80)\n",
        "            for entity in results['semantic_entities']:\n",
        "                print(f\"  Type: {entity['entity_type']:<15} | Value: {entity['entity_value']:<40} | Conf: {entity['confidence']:.3f}\")\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        logger.info(\"Training and testing completed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Process failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rxK-cNGuCeFJ",
        "outputId": "b5874517-0f66-4b62-bf8b-da99b1919bf4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1050/1050 18:33, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.184200</td>\n",
              "      <td>0.280259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.579300</td>\n",
              "      <td>0.180486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.383600</td>\n",
              "      <td>0.107141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.230700</td>\n",
              "      <td>0.078622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.165800</td>\n",
              "      <td>0.059735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.118200</td>\n",
              "      <td>0.064203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.097400</td>\n",
              "      <td>0.056653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.070600</td>\n",
              "      <td>0.054517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.060900</td>\n",
              "      <td>0.065021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.054900</td>\n",
              "      <td>0.069033</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [53/53 00:04]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "       B-DEVICE       0.84      0.76      0.80        99\n",
            "       B-DOMAIN       0.95      0.81      0.88       156\n",
            "         B-FILE       0.00      0.00      0.00         1\n",
            "     B-FUNCTION       0.98      0.97      0.98       343\n",
            "           B-IP       0.80      0.80      0.80         5\n",
            "      B-MALWARE       0.94      0.88      0.91       352\n",
            "        B-OTHER       0.97      0.77      0.86       515\n",
            "     B-PLATFORM       0.98      0.95      0.97       130\n",
            "     B-SOFTWARE       0.72      0.41      0.52        32\n",
            "  B-THREATACTOR       0.90      0.76      0.82        37\n",
            "         B-TYPE       0.99      0.84      0.91       582\n",
            "          B-URL       0.98      0.76      0.85       275\n",
            "       B-VENDOR       0.94      0.86      0.90       334\n",
            "      B-VERSION       0.85      0.50      0.63       114\n",
            "B-VULNERABILITY       0.89      0.84      0.86       118\n",
            "       I-DEVICE       0.00      0.00      0.00        18\n",
            "     I-FUNCTION       0.95      0.97      0.96       130\n",
            "      I-MALWARE       0.84      0.75      0.79       208\n",
            "     I-SOFTWARE       0.00      0.00      0.00         2\n",
            "  I-THREATACTOR       0.00      0.00      0.00         6\n",
            "         I-TYPE       0.97      0.64      0.77        47\n",
            "       I-VENDOR       0.79      0.51      0.62       381\n",
            "      I-VERSION       0.87      0.46      0.60       601\n",
            "I-VULNERABILITY       0.00      0.00      0.00        15\n",
            "              O       0.96      0.99      0.97     24036\n",
            "\n",
            "       accuracy                           0.95     28537\n",
            "      macro avg       0.72      0.61      0.66     28537\n",
            "   weighted avg       0.95      0.95      0.95     28537\n",
            "\n",
            "\n",
            "================================================================================\n",
            "EXTRACTION RESULTS\n",
            "================================================================================\n",
            "Total entities extracted: 13\n",
            "Technical IOCs: 7\n",
            "Semantic Entities: 6\n",
            "\n",
            "                                 Technical IOCs                                 \n",
            "--------------------------------------------------------------------------------\n",
            "  Type: IP              | Value: 154.216.18.83                            | Conf: 0.989\n",
            "  Type: IP              | Value: and                                      | Conf: 0.882\n",
            "  Type: IP              | Value: 104.36.83.170.                           | Conf: 0.991\n",
            "  Type: DOMAIN          | Value: aficors.com,                             | Conf: 0.933\n",
            "  Type: DOMAIN          | Value: aoc-gov.us,                              | Conf: 0.939\n",
            "  Type: DOMAIN          | Value: app-v4-mybos.com.                        | Conf: 0.911\n",
            "  Type: VULNERABILITY   | Value: CVE-2024-38196                           | Conf: 0.994\n",
            "\n",
            "                               Semantic Entities                                \n",
            "--------------------------------------------------------------------------------\n",
            "  Type: TYPE            | Value: APT                                      | Conf: 0.893\n",
            "  Type: TYPE            | Value: malware                                  | Conf: 0.961\n",
            "  Type: FUNCTION        | Value: privilege escalation.                    | Conf: 0.881\n",
            "  Type: PLATFORM        | Value: Windows                                  | Conf: 0.978\n",
            "  Type: DEVICE          | Value: targets                                  | Conf: 0.672\n",
            "  Type: VENDOR          | Value: Microsoft                                | Conf: 0.973\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYhd1EbUPxRD",
        "outputId": "cfd07b4e-2007-4b1a-facb-b6819b70fb28"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "source = \"enhanced_ioc_model_v2\"\n",
        "destination = \"/content/drive/MyDrive/enhanced_ioc_model_v2\"\n",
        "\n",
        "shutil.copytree(source, destination)\n",
        "print(f\"Model copied to: {destination}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngvmFpZP2LT",
        "outputId": "ae56149f-73ab-4458-f167-68289a8f43d1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model copied to: /content/drive/MyDrive/enhanced_ioc_model_v2\n"
          ]
        }
      ]
    }
  ]
}